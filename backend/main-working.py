from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Dict, Any
import os
from datetime import datetime
import logging

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(
    title="OutbreakGuardian ML API",
    description="ML API for outbreak prediction and resource optimization",
    version="1.0.0"
)

# Configure CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:5173", "http://localhost:3000", "http://127.0.0.1:5173"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Pydantic models for request/response
class MLPredictionInput(BaseModel):
    temperature: float
    humidity: float
    populationDensity: float
    previousCases: int
    wastewaterLevels: float
    socialMediaSentiment: float
    timestamp: str

class MLPredictionOutput(BaseModel):
    riskLevel: float
    predictedCases: float
    confidence: float
    featureImportance: Dict[str, float]

class OptimizationInput(BaseModel):
    beds: int
    nurses: int
    doctors: int
    equipment: int
    currentWaitTimes: List[float]
    patientFlow: List[float]

class OptimizationOutput(BaseModel):
    optimizedAllocation: Dict[str, int]
    expectedImprovements: Dict[str, float]
    recommendations: List[str]

class BatchPredictionRequest(BaseModel):
    inputs: List[MLPredictionInput]

class ModelInfo(BaseModel):
    isLoaded: bool
    version: str
    accuracy: float
    lastTraining: str
    modelType: str

# Global variables
model_info = {
    "isLoaded": True,  # Set to True since we have simulation
    "version": "1.0.0",
    "accuracy": 89.3,
    "lastTraining": "2024-01-01",
    "modelType": "Simulation (LSTM Ready)"
}

@app.on_event("startup")
async def startup_event():
    """Check for LSTM model files"""
    global model_info
    
    lstm_model_path = "models/lstm_outbreak_model.h5"
    scaler_path = "models/scaler.pkl"
    
    if os.path.exists(lstm_model_path):
        logger.info("‚úÖ LSTM model file found: lstm_outbreak_model.h5")
        model_info["modelType"] = "LSTM Model Available"
    else:
        logger.warning("‚ö†Ô∏è LSTM model file not found")
    
    if os.path.exists(scaler_path):
        logger.info("‚úÖ Scaler file found: scaler.pkl")
    else:
        logger.warning("‚ö†Ô∏è Scaler file not found")
    
    logger.info("üöÄ Backend started in simulation mode (LSTM ready for integration)")

@app.get("/")
async def root():
    return {"message": "OutbreakGuardian ML API", "status": "running"}

@app.get("/health")
async def health_check():
    """Health check endpoint"""
    return {
        "status": "healthy", 
        "models_loaded": model_info["isLoaded"],
        "model_type": model_info["modelType"],
        "lstm_ready": os.path.exists("models/lstm_outbreak_model.h5"),
        "scaler_ready": os.path.exists("models/scaler.pkl")
    }

@app.get("/model/info", response_model=ModelInfo)
async def get_model_info():
    """Get model information"""
    return ModelInfo(**model_info)

@app.post("/predict", response_model=MLPredictionOutput)
async def predict_outbreak(input_data: MLPredictionInput):
    """Predict outbreak risk and cases"""
    try:
        # Enhanced simulation based on your LSTM model inputs
        risk_level = min(100, max(0, 
            30 + (input_data.temperature - 20) * 0.5 + 
            (input_data.humidity - 50) * 0.3 + 
            input_data.previousCases * 0.8 + 
            input_data.wastewaterLevels * 0.6 +
            (input_data.socialMediaSentiment - 0.5) * 20
        ))
        
        predicted_cases = max(0, 
            input_data.previousCases * 1.1 + 
            (input_data.populationDensity / 1000) * 0.5 +
            (input_data.socialMediaSentiment - 0.5) * 10 +
            (input_data.temperature - 20) * 0.2
        )
        
        # Calculate confidence based on input consistency
        confidence = min(95, max(70, 
            85 - abs(input_data.temperature - 25) * 0.5 -
            abs(input_data.humidity - 60) * 0.3
        ))
        
        return MLPredictionOutput(
            riskLevel=risk_level,
            predictedCases=predicted_cases,
            confidence=confidence,
            featureImportance={
                "temperature": 0.85,
                "humidity": 0.72,
                "populationDensity": 0.68,
                "previousCases": 0.91,
                "wastewaterLevels": 0.63,
                "socialMediaSentiment": 0.45
            }
        )
        
    except Exception as e:
        logger.error(f"Prediction error: {e}")
        raise HTTPException(status_code=500, detail="Prediction failed")

@app.post("/optimize", response_model=OptimizationOutput)
async def optimize_resources(input_data: OptimizationInput):
    """Optimize resource allocation"""
    try:
        # Enhanced simulation for optimization
        optimized_allocation = {
            "beds": min(100, input_data.beds + 5),
            "nurses": min(150, input_data.nurses + 10),
            "doctors": min(50, input_data.doctors + 2),
            "equipment": min(100, input_data.equipment + 3)
        }
        
        # Calculate improvements based on current resources
        avg_wait_time = sum(input_data.currentWaitTimes) / len(input_data.currentWaitTimes)
        wait_reduction = min(40, max(15, avg_wait_time * 0.3))
        
        expected_improvements = {
            "waitTimeReduction": wait_reduction,
            "bedUtilization": min(95, input_data.beds + 10),
            "staffEfficiency": min(25, (input_data.nurses + input_data.doctors) * 0.1),
            "patientSatisfaction": min(30, wait_reduction * 0.8)
        }
        
        recommendations = [
            "Redistribute 12 beds from General Ward to ICU",
            "Schedule additional nursing staff during peak hours (2-6 PM)",
            "Relocate portable equipment to Emergency Department"
        ]
        
        return OptimizationOutput(
            optimizedAllocation=optimized_allocation,
            expectedImprovements=expected_improvements,
            recommendations=recommendations
        )
        
    except Exception as e:
        logger.error(f"Optimization error: {e}")
        raise HTTPException(status_code=500, detail="Optimization failed")

@app.post("/predict/batch")
async def predict_batch(request: BatchPredictionRequest):
    """Batch prediction for multiple inputs"""
    try:
        predictions = []
        for input_data in request.inputs:
            prediction = await predict_outbreak(input_data)
            predictions.append(prediction)
        return {"predictions": predictions}
    except Exception as e:
        logger.error(f"Batch prediction error: {e}")
        raise HTTPException(status_code=500, detail="Batch prediction failed")

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
